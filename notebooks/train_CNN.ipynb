{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7ecc294",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-01 22:02:56.446404: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-01 22:02:57.400619: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-01 22:02:58.526296: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-01 22:02:58.526424: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-01 22:02:58.526436: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2022-10-01 22:03:00.616250: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-10-01 22:03:00.616354: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-10-01 22:03:00.617928: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 106\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcnnmodel\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    103\u001b[0m     os\u001b[38;5;241m.\u001b[39mmkdir(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcnnmodel\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    105\u001b[0m hist \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m--> 106\u001b[0m     \u001b[43mtrain_x\u001b[49m,\n\u001b[1;32m    107\u001b[0m     train_y,\n\u001b[1;32m    108\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m    109\u001b[0m     epochs\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m    110\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mModelCheckpoint(filepath, monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, save_weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m)],\n\u001b[1;32m    111\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39m(val_x,val_y)\n\u001b[1;32m    112\u001b[0m )\n\u001b[1;32m    114\u001b[0m loss, mae \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(\n\u001b[1;32m    115\u001b[0m     test_x,\n\u001b[1;32m    116\u001b[0m     test_y,\n\u001b[1;32m    117\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size\n\u001b[1;32m    118\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_x' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Conv3D, MaxPooling3D, Dropout, Dense, Flatten, Activation, BatchNormalization\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import h5py\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "#Load the data of training set and test set. The last ~10% of the training set is split as the validation set\n",
    "def load_file():\n",
    "    with open('train_grids.pkl','rb') as f:\n",
    "        train_grids = pickle.load(f)\n",
    "    with open('train_label.pkl','rb') as f:\n",
    "        train_label = pickle.load(f)\n",
    "    assert train_grids.shape[0] == len(train_label)\n",
    "    val_x = train_grids[41000:]\n",
    "    val_y = train_label[41000:]\n",
    "    mask1 = list(range(41000))\n",
    "    np.random.seed(1234)\n",
    "    np.random.shuffle(mask1)\n",
    "    train_x = train_grids[mask1]\n",
    "    train_y = train_label[mask1]\n",
    "\n",
    "    mask2 = list(range(len(val_y)))\n",
    "    np.random.shuffle(mask2)\n",
    "    val_x = val_x[mask2]\n",
    "    val_y = val_y[mask2]\n",
    "    with open('core_grids.pkl','rb') as f:\n",
    "        core_grids = pickle.load(f)\n",
    "    with open('core_label.pkl','rb') as f:\n",
    "        core_label = pickle.load(f)\n",
    "    test_x = core_grids\n",
    "    test_y = core_label\n",
    "\n",
    "    return (train_x, train_y, val_x, val_y, test_x, test_y)\n",
    "\n",
    "# train_x, train_y, val_x, val_y, test_x, test_y = load_file()\n",
    "\n",
    "#Normalize the label\n",
    "# if True:\n",
    "#     train_y = train_y / 15.0\n",
    "#     test_y = test_y / 15.0\n",
    "#     val_y = val_y / 15.0\n",
    "\n",
    "# In[ ]:\n",
    "# import argparse\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--batch', '-b', default=64, type=int)\n",
    "# parser.add_argument('--dropout', '-d', default=0.5, type=float)\n",
    "# parser.add_argument('--lr', default=0.004, type=float)\n",
    "# args = parser.parse_args()\n",
    "\n",
    "batch_size = 64\n",
    "dropout=0.5\n",
    "lr=0.004\n",
    "epoch = 200\n",
    "#Build the 3d cnn model. \n",
    "model = tf.keras.Sequential([\n",
    "    Conv3D(7,kernel_size=(1,1,1),input_shape=(300,300,300,94),strides=(1,1,1)),\n",
    "    BatchNormalization(),  \n",
    "    Activation(tf.nn.relu),\n",
    "    Conv3D(7,kernel_size=(3,3,3)),\n",
    "    BatchNormalization(),  \n",
    "    Activation(tf.nn.relu),\n",
    "    Conv3D(7,kernel_size=(3,3,3)),\n",
    "    BatchNormalization(),\n",
    "    Activation(tf.nn.relu),\n",
    "    Conv3D(28,kernel_size=(1,1,1)),\n",
    "    BatchNormalization(),  \n",
    "    Activation(tf.nn.relu),\n",
    "    Conv3D(56,kernel_size=(3,3,3),padding='same'),\n",
    "    BatchNormalization(),  \n",
    "    Activation(tf.nn.relu),\n",
    "    MaxPooling3D(pool_size=2),\n",
    "    Conv3D(112,kernel_size=(3,3,3),padding='same'),\n",
    "    BatchNormalization(),  \n",
    "    Activation(tf.nn.relu),\n",
    "    MaxPooling3D(pool_size=2),\n",
    "    Conv3D(224,kernel_size=(3,3,3),padding='same'),\n",
    "    BatchNormalization(),  \n",
    "    Activation(tf.nn.relu),\n",
    "    MaxPooling3D(pool_size=2),\n",
    "    Flatten(),\n",
    "    Dense(256),\n",
    "    BatchNormalization(),\n",
    "    Activation(tf.nn.relu),\n",
    "    Dropout(dropout),\n",
    "    Dense(1,kernel_regularizer=tf.keras.regularizers.l2(0.01))]\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(lr),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "filepath = 'cnnmodel/weights_{epoch:03d}-{val_loss:.4f}.h5'\n",
    "if not os.path.exists('cnnmodel'):\n",
    "    os.mkdir('cnnmodel')\n",
    "\n",
    "hist = model.fit(\n",
    "    train_x,\n",
    "    train_y,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epoch,\n",
    "    callbacks=[tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True, mode='min')],\n",
    "    validation_data=(val_x,val_y)\n",
    ")\n",
    "\n",
    "loss, mae = model.evaluate(\n",
    "    test_x,\n",
    "    test_y,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dd9ec36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f041ccaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen??"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
