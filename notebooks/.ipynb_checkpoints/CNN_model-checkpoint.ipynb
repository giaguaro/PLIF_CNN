{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d725f43a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f293d77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "388b5016",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21d70f30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e2958d71",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Default process group has not been initialized, please make sure to call init_process_group.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [57], line 75\u001b[0m\n\u001b[1;32m     72\u001b[0m train_data, validate_data \u001b[38;5;241m=\u001b[39mtrain_data\u001b[38;5;241m+\u001b[39mvalidate_data[:\u001b[38;5;241m5000\u001b[39m], validate_data[\u001b[38;5;241m5000\u001b[39m:]     \n\u001b[1;32m     74\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m CNNDataLoader(train_data)\n\u001b[0;32m---> 75\u001b[0m train_sampler \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistributed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDistributedSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m validate_dataset \u001b[38;5;241m=\u001b[39m CNNDataLoader(validate_data)\n\u001b[1;32m     78\u001b[0m val_sampler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/plifs/lib/python3.8/site-packages/torch/utils/data/distributed.py:67\u001b[0m, in \u001b[0;36mDistributedSampler.__init__\u001b[0;34m(self, dataset, num_replicas, rank, shuffle, seed, drop_last)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dist\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequires distributed package to be available\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 67\u001b[0m     num_replicas \u001b[38;5;241m=\u001b[39m \u001b[43mdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_world_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rank \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dist\u001b[38;5;241m.\u001b[39mis_available():\n",
      "File \u001b[0;32m~/.conda/envs/plifs/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:867\u001b[0m, in \u001b[0;36mget_world_size\u001b[0;34m(group)\u001b[0m\n\u001b[1;32m    864\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _rank_not_in_group(group):\n\u001b[1;32m    865\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 867\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_group_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/plifs/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:325\u001b[0m, in \u001b[0;36m_get_group_size\u001b[0;34m(group)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;124;03mHelper that gets a given group's world size.\u001b[39;00m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m group \u001b[38;5;129;01mis\u001b[39;00m GroupMember\u001b[38;5;241m.\u001b[39mWORLD \u001b[38;5;129;01mor\u001b[39;00m group \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 325\u001b[0m     default_pg \u001b[38;5;241m=\u001b[39m \u001b[43m_get_default_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default_pg\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m group\u001b[38;5;241m.\u001b[39msize()\n",
      "File \u001b[0;32m~/.conda/envs/plifs/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:429\u001b[0m, in \u001b[0;36m_get_default_group\u001b[0;34m()\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;124;03mGetting the default process group created by init_process_group\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_initialized():\n\u001b[0;32m--> 429\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    430\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDefault process group has not been initialized, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    431\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease make sure to call init_process_group.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    432\u001b[0m     )\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m GroupMember\u001b[38;5;241m.\u001b[39mWORLD\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Default process group has not been initialized, please make sure to call init_process_group."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602b342b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c843a05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the network, optimizer and objective func\n",
    "#*************************************\n",
    "cnn = CNN()\n",
    "cnn = torch.nn.DataParallel(cnn, device_ids=[0, 1, 2, 3])\n",
    "cnn.to(torch.device('cuda:0'))\n",
    "if loadmodel: # load checkpoint if needed\n",
    "    print(\"Loading existing checkpoint...\")\n",
    "    cnn.load_state_dict(torch.load(loadmodelpath))\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=lr)\n",
    "#criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(299,dtype=torch.float,device=device))  #\n",
    "criterion = nn.MSELoss() #nn.BCEWithLogitsLoss()  ##nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97074da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train!\n",
    "#*************************************\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "list_of_auc = []\n",
    "\n",
    "print('{:%Y-%m-%d %H:%M:%S} Starting training...'.format(datetime.now()))\n",
    "start_time = time.monotonic()\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    np.random.seed(epoch)\n",
    "    random.seed(epoch)\n",
    "    if args.distributed: \n",
    "        train_loader.sampler.set_epoch(epoch)\n",
    "    train(cnn,epoch, train_loader)\n",
    "elapsed_time = time.monotonic() - start_time\n",
    "print('Training time taken:',time_taken(elapsed_time))\n",
    "\n",
    "if savemodel_interval == 0 and savemodel:\n",
    "    torch.save(cnn.state_dict(), \n",
    "       '{}/{:%Y-%m-%d_%H-%M-%S}_model_epoch{}.pth'.format(savemodeldir,datetime.now(),num_epochs))\n",
    "    print('model saved at epoch{}'.format(num_epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15a2e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import builtins\n",
    "import argparse\n",
    "import torch.distributed as dist\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transform\n",
    "import random\n",
    "from sklearn import metrics\n",
    "import matplotlib.pylab as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import glob\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "# Some utility functions\n",
    "#*************************************\n",
    "def time_taken(elapsed):\n",
    "    \"\"\"To format time taken in hh:mm:ss. Use with time.monotic()\"\"\"\n",
    "    m, s = divmod(elapsed, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    return \"%d:%02d:%02d\" % (h, m, s)\n",
    "\n",
    "def mydate() :\n",
    "    return (datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "\n",
    "# Read/write directory parameters\n",
    "#*************************************\n",
    "datadir = 'training_data'\n",
    "savemodeldir = 'new_model'\n",
    "loadmodelpath = 'model/2018-10-30_03-12-21_model_epoch30.pth'\n",
    "\n",
    "# Pytorch parameters\n",
    "#*************************************\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "savemodel = True\n",
    "savemodel_interval = 1  #if 0 (and savemodel=True) will only save model at the end of entire training\n",
    "loadmodel = False\n",
    "\n",
    "# Training parameters\n",
    "#*************************************\n",
    "batch_size = 2\n",
    "num_epochs = 200\n",
    "lr = 1e-4\n",
    "log_interval = 10\n",
    "random.seed(1234) #for dataset splitting set to None of leave blank if do not need to preserve random order\n",
    "\n",
    "# Preprocessing parameters\n",
    "#*************************************\n",
    "bins = 48\n",
    "hrange = 24\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    # input size - the number of \"classes\"\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv3d(94, 32, kernel_size=5, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv3d(32, 64, kernel_size=5, stride=1, padding=0),\n",
    "            #nn.BatchNorm3d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=2, stride=2))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv3d(64, 128, kernel_size=5, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=8, stride=2))\n",
    "        self.fc0 = nn.Linear(746496,1024)\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(1024, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5))\n",
    "        self.fc2 = nn.Linear(100, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print(\"in\",x.shape)\n",
    "        out = self.layer1(x)\n",
    "        #print(out.shape)\n",
    "        out = self.layer2(out)\n",
    "        #print(out.shape)\n",
    "        out = self.layer3(out)\n",
    "        #print(out.shape)\n",
    "        \n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        \n",
    "        out = self.fc0(out)\n",
    "        #print(out.shape)\n",
    "        out = self.fc1(out)\n",
    "        #print(out.shape)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        #print(out.type())\n",
    "        return out\n",
    "    \n",
    "CNN()\n",
    "\n",
    "class CNNDataLoader(Dataset):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_pickle (string): Directory with to pickle file processed tensor data\n",
    "            master_file (string): Path to the master csv file with annotations. Column 'kd\\ki' has labels.\n",
    "        \"\"\"\n",
    "\n",
    "        self.data = data\n",
    "    \n",
    "  \n",
    "                \n",
    "    def __len__(self):\n",
    "           return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        grids_path, label_path = self.data[idx]\n",
    "\n",
    "        with open(label_path,'rb') as f: \n",
    "            label = pickle.load(f)\n",
    "            \n",
    "        with open(grids_path,'rb') as f: \n",
    "            grid = pickle.load(f)\n",
    "        \n",
    "        #torch.unsqueeze(grid, dim=0)\n",
    "        a_grid = grid[0].to_dense() if grid.shape==(1, 200, 200, 200, 94) else grid.to_dense()\n",
    "        try: a_label = torch.tensor(label[0])\n",
    "        except: a_label = torch.tensor(label)\n",
    "        \n",
    "        return a_grid, a_label\n",
    "\n",
    "\n",
    "# def collate_fn(data):\n",
    "#     \"\"\"\n",
    "#        data: is a list of tuples with (example, label, length)\n",
    "#              where 'example' is a tensor of arbitrary shape\n",
    "#              and label/length are scalars\n",
    "#     \"\"\"\n",
    "#     _, labels, lengths = zip(*data)\n",
    "#     max_len = max(lengths)\n",
    "#     n_ftrs = data[0][0].size(1)\n",
    "#     features = torch.zeros((len(data), max_len, n_ftrs))\n",
    "#     labels = torch.tensor(labels)\n",
    "#     lengths = torch.tensor(lengths)\n",
    "\n",
    "#     for i in range(len(data)):\n",
    "#         j, k = data[i][0].size(0), data[i][0].size(1)\n",
    "#         features[i] = torch.cat([data[i][0], torch.zeros((max_len - j, k))])\n",
    "\n",
    "#     return features.float(), labels.long(), lengths.long()\n",
    "\n",
    "    \n",
    "    \n",
    "    #Make calls to the dataloader\n",
    "#     for tensor_batch, label_batch in collected_batch:\n",
    "#         print(\"Batch of tensors has shape: \", tensor_batch.shape)\n",
    "#         print(\"Batch of labels has shape: \", label_batch)\n",
    "\n",
    "# Define the training cycle (100% teacher forcing for now)\n",
    "#*************************************\n",
    "def train(model,epoch, train_loader):\n",
    "    model.train() #put in training mode\n",
    "    \n",
    "    for step, (inp,target) in enumerate(training_loader):\n",
    "        target = target.float()\n",
    "        inp, target = inp.to(device), target.to(device)\n",
    "        inp,target = inp.cuda(), target.cuda()\n",
    "        inp = inp.view(inp.shape[0],-1,200,200,200)\n",
    "        \n",
    "        # Forward + Backward + Optimize\n",
    "        outputs = model(inp)\n",
    "        #print(outputs,target)\n",
    "        loss = criterion(outputs, target)\n",
    "        #print(loss.item())\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "                \n",
    "    print ('{:%Y-%m-%d %H:%M:%S} Epoch [{}/{}], Step [{}/{}] Loss: {:.6f}'.format( \n",
    "        datetime.now(), epoch+1, num_epochs, step+1, len(train_data)//batch_size, loss.item()))\n",
    "    \n",
    "    list_of_losses.append(loss.item())\n",
    "    \n",
    "    if args.rank == 0:\n",
    "        evaluate_mse(model)\n",
    "                   \n",
    "    if savemodel_interval != 0 and savemodel:\n",
    "        if (epoch+1) % savemodel_interval == 0:\n",
    "            torch.save(model.state_dict(),\n",
    "                       '{}/{:%Y-%m-%d_%H-%M-%S}_model_epoch{}_step{}.pth'.format(savemodeldir,datetime.now(),epoch+1,step+1))\n",
    "            print('model saved at epoch{} step{}'.format(epoch+1,step+1))\n",
    "\n",
    "\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inp, target in validation_loader:\n",
    "            inp, target = inp.to(device), target.to(device)\n",
    "            inp = inp.view(inp.shape[0],-1,200,200,200)\n",
    "\n",
    "            outputs = model(inp)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == torch.max(target, 1)[1]).sum().item()\n",
    "\n",
    "        print('Accuracy of the model on the validation set: {} %'.format(100 * correct / total))\n",
    "        \n",
    "        \n",
    "def evaluate_mse(model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = []\n",
    "        targets = []\n",
    "        for step, (inp, target) in enumerate(validation_loader):\n",
    "            inp = inp.to(device)\n",
    "            inp = inp.view(inp.shape[0],-1,200,200,200)\n",
    "            outputs = model(inp)\n",
    "            outputs_numpy = outputs.detach().cpu().numpy()\n",
    "            targets_numpy = target.numpy()\n",
    "            for i in range(outputs_numpy.shape[0]):\n",
    "                out.append(outputs_numpy.item(i))\n",
    "                targets.append(targets_numpy.item(i))\n",
    "        print(out)\n",
    "        print(targets)\n",
    "        auc = auc_curve(out,targets)\n",
    "        list_of_auc.append(auc)\n",
    "            \n",
    "def auc_curve(output,target):\n",
    "    \"\"\"Plot a ROC curve\"\"\"\n",
    "    fpr, tpr, _ = metrics.roc_curve(target,  output)\n",
    "    auc = metrics.roc_auc_score(target, output)\n",
    "    plt.figure() \n",
    "    plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "    plt.legend(loc=4)\n",
    "    plt.show()\n",
    "    plt.savefig('auc.png')\n",
    "    return(auc)\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--net', default='resnet18', type=str)\n",
    "    parser.add_argument('--lr', default=1e-3, type=float, help='learning rate')\n",
    "    parser.add_argument('--batch_size', default=16, type=int, help='batch size per GPU')\n",
    "    parser.add_argument('--gpu', default=None, type=int)\n",
    "    parser.add_argument('--start_epoch', default=0, type=int, \n",
    "                        help='start epoch number (useful on restarts)')\n",
    "    parser.add_argument('--epochs', default=10, type=int, help='number of total epochs to run')\n",
    "    # DDP configs:\n",
    "    parser.add_argument('--world-size', default=-1, type=int, \n",
    "                        help='number of nodes for distributed training')\n",
    "    parser.add_argument('--rank', default=-1, type=int, \n",
    "                        help='node rank for distributed training')\n",
    "    parser.add_argument('--dist-url', default='env://', type=str, \n",
    "                        help='url used to set up distributed training')\n",
    "    parser.add_argument('--dist-backend', default='nccl', type=str, \n",
    "                        help='distributed backend')\n",
    "    parser.add_argument('--local_rank', default=-1, type=int, \n",
    "                        help='local rank for distributed training')\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    # DDP setting\n",
    "    if \"WORLD_SIZE\" in os.environ:\n",
    "        args.world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "    args.distributed = args.world_size > 1\n",
    "    ngpus_per_node = torch.cuda.device_count()\n",
    "\n",
    "    if args.distributed:\n",
    "        if args.local_rank != -1: # for torch.distributed.launch\n",
    "            args.rank = args.local_rank\n",
    "            args.gpu = args.local_rank\n",
    "        elif 'SLURM_PROCID' in os.environ: # for slurm scheduler\n",
    "            args.rank = int(os.environ['SLURM_PROCID'])\n",
    "            args.gpu = args.rank % torch.cuda.device_count()\n",
    "        dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n",
    "                                world_size=args.world_size, rank=args.rank)\n",
    "\n",
    "    # suppress printing if not on master gpu\n",
    "    if args.rank!=0:\n",
    "        def print_pass(*args):\n",
    "            pass\n",
    "        builtins.print = print_pass\n",
    "       \n",
    "    ### model ###\n",
    "    model = CNN()\n",
    "    if args.distributed:\n",
    "        # For multiprocessing distributed, DistributedDataParallel constructor\n",
    "        # should always set the single device scope, otherwise,\n",
    "        # DistributedDataParallel will use all available devices.\n",
    "        if args.gpu is not None:\n",
    "            torch.cuda.set_device(args.gpu)\n",
    "            model.cuda(args.gpu)\n",
    "            model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])\n",
    "            model_without_ddp = model.module\n",
    "        else:\n",
    "            model.cuda()\n",
    "            model = torch.nn.parallel.DistributedDataParallel(model)\n",
    "            model_without_ddp = model.module\n",
    "    else:\n",
    "        raise NotImplementedError(\"Only DistributedDataParallel is supported.\")\n",
    "        \n",
    "    \n",
    "    ### resume training if necessary ###\n",
    "    if args.resume:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    ### main loop ###\n",
    "    for epoch in range(args.start_epoch, args.epochs):\n",
    "        np.random.seed(epoch)\n",
    "        random.seed(epoch)\n",
    "        # fix sampling seed such that each gpu gets different part of dataset\n",
    "        if args.distributed: \n",
    "            train_loader.sampler.set_epoch(epoch)\n",
    "        \n",
    "        # adjust lr if needed #\n",
    "        \n",
    "        train_one_epoch(train_loader, model, criterion, optimizer, epoch, args)\n",
    "        if args.rank == 0: # only val and save on master node\n",
    "            validate(val_loader, model, criterion, epoch, args)\n",
    "            # save checkpoint if needed #\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    os.chdir(\"/groups/cherkasvgrp/share/progressive_docking/hmslati/plif_cnn/\")\n",
    "    with open(\"train_data.pkl\",'rb') as f: \n",
    "        train_data=pickle.load(f)\n",
    "    with open(\"validate_data.pkl\",'rb') as f: \n",
    "        validate_data=pickle.load(f)\n",
    "    with open(\"test_data.pkl\",'rb') as f: \n",
    "        test_data=pickle.load(f)\n",
    "        \n",
    "    train_data, validate_data =train_data+validate_data[:5000], validate_data[5000:]     \n",
    "\n",
    "    train_dataset = CNNDataLoader(train_data)\n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset, shuffle=True)\n",
    "    \n",
    "    validate_dataset = CNNDataLoader(validate_data)\n",
    "    val_sampler = None\n",
    "    \n",
    "    test_dataset = CNNDataLoader(test_data)\n",
    "    \n",
    "    #Initiate the dataloader\n",
    "    training_loader = DataLoader(train_dataset,num_workers=10, pin_memory=True, batch_size=2, \n",
    "                                 shuffle=(train_sampler is None), sampler=train_sampler)\n",
    "    validation_loader = DataLoader(validate_dataset,num_workers=10, pin_memory=True, batch_size=2, \n",
    "                                   shuffle=(val_sampler is None),sampler=val_sampler)\n",
    "    testing_loader = DataLoader(test_dataset,num_workers=10, pin_memory=True, batch_size=2, shuffle=True)\n",
    "    args = parse_args()\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655be736",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
