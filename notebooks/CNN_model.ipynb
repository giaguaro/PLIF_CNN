{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d527b091",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--net NET] [--lr LR]\n",
      "                             [--batch_size BATCH_SIZE] [--gpu GPU]\n",
      "                             [--start_epoch START_EPOCH] [--epochs EPOCHS]\n",
      "                             [--world-size WORLD_SIZE] [--rank RANK]\n",
      "                             [--dist-url DIST_URL]\n",
      "                             [--dist-backend DIST_BACKEND]\n",
      "                             [--local_rank LOCAL_RANK]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/hmslati/.local/share/jupyter/runtime/kernel-ca943c27-0360-4497-98a8-eeed14edf067.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hmslati/.conda/envs/plifs/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3386: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import builtins\n",
    "import argparse\n",
    "import torch.distributed as dist\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transform\n",
    "import random\n",
    "from sklearn import metrics\n",
    "import matplotlib.pylab as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import glob\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "# Some utility functions\n",
    "#*************************************\n",
    "def time_taken(elapsed):\n",
    "    \"\"\"To format time taken in hh:mm:ss. Use with time.monotic()\"\"\"\n",
    "    m, s = divmod(elapsed, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    return \"%d:%02d:%02d\" % (h, m, s)\n",
    "\n",
    "def mydate() :\n",
    "    return (datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "\n",
    "# Read/write directory parameters\n",
    "#*************************************\n",
    "datadir = 'training_data'\n",
    "savemodeldir = 'new_model'\n",
    "loadmodelpath = 'model/2018-10-30_03-12-21_model_epoch30.pth'\n",
    "\n",
    "# Pytorch parameters\n",
    "#*************************************\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "savemodel = True\n",
    "savemodel_interval = 1  #if 0 (and savemodel=True) will only save model at the end of entire training\n",
    "loadmodel = False\n",
    "\n",
    "# Training parameters\n",
    "#*************************************\n",
    "batch_size = 2\n",
    "num_epochs = 200\n",
    "lr = 1e-4\n",
    "log_interval = 10\n",
    "random.seed(1234) #for dataset splitting set to None of leave blank if do not need to preserve random order\n",
    "\n",
    "# Preprocessing parameters\n",
    "#*************************************\n",
    "bins = 48\n",
    "hrange = 24\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    # input size - the number of \"classes\"\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv3d(94, 32, kernel_size=5, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv3d(32, 64, kernel_size=5, stride=1, padding=0),\n",
    "            #nn.BatchNorm3d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=2, stride=2))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv3d(64, 128, kernel_size=5, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=8, stride=2))\n",
    "        self.fc0 = nn.Linear(746496,1024)\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(1024, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5))\n",
    "        self.fc2 = nn.Linear(100, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print(\"in\",x.shape)\n",
    "        out = self.layer1(x)\n",
    "        #print(out.shape)\n",
    "        out = self.layer2(out)\n",
    "        #print(out.shape)\n",
    "        out = self.layer3(out)\n",
    "        #print(out.shape)\n",
    "        \n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        \n",
    "        out = self.fc0(out)\n",
    "        #print(out.shape)\n",
    "        out = self.fc1(out)\n",
    "        #print(out.shape)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        #print(out.type())\n",
    "        return out\n",
    "    \n",
    "CNN()\n",
    "\n",
    "class CNNDataLoader(Dataset):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_pickle (string): Directory with to pickle file processed tensor data\n",
    "            master_file (string): Path to the master csv file with annotations. Column 'kd\\ki' has labels.\n",
    "        \"\"\"\n",
    "\n",
    "        self.data = data\n",
    "    \n",
    "  \n",
    "                \n",
    "    def __len__(self):\n",
    "           return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        grids_path, label_path = self.data[idx]\n",
    "\n",
    "        with open(label_path,'rb') as f: \n",
    "            label = pickle.load(f)\n",
    "            \n",
    "        with open(grids_path,'rb') as f: \n",
    "            grid = pickle.load(f)\n",
    "        \n",
    "        #torch.unsqueeze(grid, dim=0)\n",
    "        a_grid = grid[0].to_dense() if grid.shape==(1, 200, 200, 200, 94) else grid.to_dense()\n",
    "        try: a_label = torch.tensor(label[0])\n",
    "        except: a_label = torch.tensor(label)\n",
    "        \n",
    "        return a_grid, a_label\n",
    "\n",
    "\n",
    "# def collate_fn(data):\n",
    "#     \"\"\"\n",
    "#        data: is a list of tuples with (example, label, length)\n",
    "#              where 'example' is a tensor of arbitrary shape\n",
    "#              and label/length are scalars\n",
    "#     \"\"\"\n",
    "#     _, labels, lengths = zip(*data)\n",
    "#     max_len = max(lengths)\n",
    "#     n_ftrs = data[0][0].size(1)\n",
    "#     features = torch.zeros((len(data), max_len, n_ftrs))\n",
    "#     labels = torch.tensor(labels)\n",
    "#     lengths = torch.tensor(lengths)\n",
    "\n",
    "#     for i in range(len(data)):\n",
    "#         j, k = data[i][0].size(0), data[i][0].size(1)\n",
    "#         features[i] = torch.cat([data[i][0], torch.zeros((max_len - j, k))])\n",
    "\n",
    "#     return features.float(), labels.long(), lengths.long()\n",
    "\n",
    "    \n",
    "    \n",
    "    #Make calls to the dataloader\n",
    "#     for tensor_batch, label_batch in collected_batch:\n",
    "#         print(\"Batch of tensors has shape: \", tensor_batch.shape)\n",
    "#         print(\"Batch of labels has shape: \", label_batch)\n",
    "\n",
    "# Define the training cycle (100% teacher forcing for now)\n",
    "#*************************************\n",
    "def train(model,epoch, train_loader):\n",
    "    model.train() #put in training mode\n",
    "    \n",
    "    for step, (inp,target) in enumerate(training_loader):\n",
    "        target = target.float()\n",
    "        inp, target = inp.to(device), target.to(device)\n",
    "        inp,target = inp.cuda(), target.cuda()\n",
    "        inp = inp.view(inp.shape[0],-1,200,200,200)\n",
    "        \n",
    "        # Forward + Backward + Optimize\n",
    "        outputs = model(inp)\n",
    "        #print(outputs,target)\n",
    "        loss = criterion(outputs, target)\n",
    "        #print(loss.item())\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "                \n",
    "    print ('{:%Y-%m-%d %H:%M:%S} Epoch [{}/{}], Step [{}/{}] Loss: {:.6f}'.format( \n",
    "        datetime.now(), epoch+1, num_epochs, step+1, len(train_data)//batch_size, loss.item()))\n",
    "    \n",
    "    list_of_losses.append(loss.item())\n",
    "    \n",
    "    if args.rank == 0:\n",
    "        evaluate_mse(model)\n",
    "                   \n",
    "    if savemodel_interval != 0 and savemodel:\n",
    "        if (epoch+1) % savemodel_interval == 0:\n",
    "            torch.save(model.state_dict(),\n",
    "                       '{}/{:%Y-%m-%d_%H-%M-%S}_model_epoch{}_step{}.pth'.format(savemodeldir,datetime.now(),epoch+1,step+1))\n",
    "            print('model saved at epoch{} step{}'.format(epoch+1,step+1))\n",
    "\n",
    "# Initialize the network, optimizer and objective func\n",
    "#*************************************\n",
    "cnn = CNN()\n",
    "if loadmodel: # load checkpoint if needed\n",
    "    print(\"Loading existing checkpoint...\")\n",
    "    cnn.load_state_dict(torch.load(loadmodelpath))\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=lr)\n",
    "#criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(299,dtype=torch.float,device=device))  #\n",
    "criterion = nn.MSELoss() #nn.BCEWithLogitsLoss()  ##nn.MSELoss()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inp, target in validation_loader:\n",
    "            inp, target = inp.to(device), target.to(device)\n",
    "            inp = inp.view(inp.shape[0],-1,200,200,200)\n",
    "\n",
    "            outputs = model(inp)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == torch.max(target, 1)[1]).sum().item()\n",
    "\n",
    "        print('Accuracy of the model on the validation set: {} %'.format(100 * correct / total))\n",
    "        \n",
    "        \n",
    "def evaluate_mse(model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = []\n",
    "        targets = []\n",
    "        for step, (inp, target) in enumerate(validation_loader):\n",
    "            inp = inp.to(device)\n",
    "            inp = inp.view(inp.shape[0],-1,200,200,200)\n",
    "            outputs = model(inp)\n",
    "            outputs_numpy = outputs.detach().cpu().numpy()\n",
    "            targets_numpy = target.numpy()\n",
    "            for i in range(outputs_numpy.shape[0]):\n",
    "                out.append(outputs_numpy.item(i))\n",
    "                targets.append(targets_numpy.item(i))\n",
    "        print(out)\n",
    "        print(targets)\n",
    "        auc = auc_curve(out,targets)\n",
    "        list_of_auc.append(auc)\n",
    "            \n",
    "def auc_curve(output,target):\n",
    "    \"\"\"Plot a ROC curve\"\"\"\n",
    "    fpr, tpr, _ = metrics.roc_curve(target,  output)\n",
    "    auc = metrics.roc_auc_score(target, output)\n",
    "    plt.figure() \n",
    "    plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "    plt.legend(loc=4)\n",
    "    plt.show()\n",
    "    plt.savefig('auc.png')\n",
    "    return(auc)\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--net', default='cnn', type=str)\n",
    "    parser.add_argument('--lr', default=1e-4, type=float, help='learning rate')\n",
    "    parser.add_argument('--batch_size', default=2, type=int, help='batch size per GPU')\n",
    "    parser.add_argument('--gpu', default=None, type=int)\n",
    "    parser.add_argument('--start_epoch', default=0, type=int, \n",
    "                        help='start epoch number (useful on restarts)')\n",
    "    parser.add_argument('--epochs', default=200, type=int, help='number of total epochs to run')\n",
    "    # DDP configs:\n",
    "    parser.add_argument('--world-size', default=-1, type=int, \n",
    "                        help='number of nodes for distributed training')\n",
    "    parser.add_argument('--rank', default=-1, type=int, \n",
    "                        help='node rank for distributed training')\n",
    "    parser.add_argument('--dist-url', default='env://', type=str, \n",
    "                        help='url used to set up distributed training')\n",
    "    parser.add_argument('--dist-backend', default='nccl', type=str, \n",
    "                        help='distributed backend')\n",
    "    parser.add_argument('--local_rank', default=-1, type=int, \n",
    "                        help='local rank for distributed training')\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    # DDP setting\n",
    "    if \"WORLD_SIZE\" in os.environ:\n",
    "        args.world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "    args.distributed = args.world_size > 1\n",
    "    ngpus_per_node = torch.cuda.device_count()\n",
    "\n",
    "    if args.distributed:\n",
    "        if args.local_rank != -1: # for torch.distributed.launch\n",
    "            args.rank = args.local_rank\n",
    "            args.gpu = args.local_rank\n",
    "        elif 'SLURM_PROCID' in os.environ: # for slurm scheduler\n",
    "            args.rank = int(os.environ['SLURM_PROCID'])\n",
    "            args.gpu = args.rank % torch.cuda.device_count()\n",
    "        dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n",
    "                                world_size=args.world_size, rank=args.rank)\n",
    "\n",
    "    # suppress printing if not on master gpu\n",
    "    if args.rank!=0:\n",
    "        def print_pass(*args):\n",
    "            pass\n",
    "        builtins.print = print_pass\n",
    "       \n",
    "    ### model ###\n",
    "    model = cnn\n",
    "    if args.distributed:\n",
    "        # For multiprocessing distributed, DistributedDataParallel constructor\n",
    "        # should always set the single device scope, otherwise,\n",
    "        # DistributedDataParallel will use all available devices.\n",
    "        if args.gpu is not None:\n",
    "            torch.cuda.set_device(args.gpu)\n",
    "            model.cuda(args.gpu)\n",
    "            model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])\n",
    "            model_without_ddp = model.module\n",
    "        else:\n",
    "            model.cuda()\n",
    "            model = torch.nn.parallel.DistributedDataParallel(model)\n",
    "            model_without_ddp = model.module\n",
    "    else:\n",
    "        raise NotImplementedError(\"Only DistributedDataParallel is supported.\")\n",
    "        \n",
    "    \n",
    "    ### resume training if necessary ###\n",
    "    if args.resume:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    os.chdir(\"/groups/cherkasvgrp/share/progressive_docking/hmslati/plif_cnn/\")\n",
    "    with open(\"train_data.pkl\",'rb') as f: \n",
    "        train_data=pickle.load(f)\n",
    "    with open(\"validate_data.pkl\",'rb') as f: \n",
    "        validate_data=pickle.load(f)\n",
    "    with open(\"test_data.pkl\",'rb') as f: \n",
    "        test_data=pickle.load(f)\n",
    "        \n",
    "    train_data, validate_data =train_data+validate_data[:5000], validate_data[5000:]     \n",
    "\n",
    "    train_dataset = CNNDataLoader(train_data)\n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset, shuffle=True)\n",
    "    \n",
    "    validate_dataset = CNNDataLoader(validate_data)\n",
    "    val_sampler = None\n",
    "    \n",
    "    test_dataset = CNNDataLoader(test_data)\n",
    "    \n",
    "    #Initiate the dataloader\n",
    "    training_loader = DataLoader(train_dataset,num_workers=10, pin_memory=True, batch_size=2, \n",
    "                                 shuffle=(train_sampler is None), sampler=train_sampler)\n",
    "    validation_loader = DataLoader(validate_dataset,num_workers=10, pin_memory=True, batch_size=2, \n",
    "                                   shuffle=(val_sampler is None),sampler=val_sampler)\n",
    "    testing_loader = DataLoader(test_dataset,num_workers=10, pin_memory=True, batch_size=2, shuffle=True)\n",
    "    \n",
    "    \n",
    "    ### main loop ###\n",
    "       # Train!\n",
    "    #*************************************\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    list_of_auc = []\n",
    "\n",
    "    print('{:%Y-%m-%d %H:%M:%S} Starting training...'.format(datetime.now()))\n",
    "    start_time = time.monotonic()\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        np.random.seed(epoch)\n",
    "        random.seed(epoch)\n",
    "        if args.distributed: \n",
    "            train_loader.sampler.set_epoch(epoch)\n",
    "        train(cnn,epoch, train_loader)\n",
    "    elapsed_time = time.monotonic() - start_time\n",
    "    print('Training time taken:',time_taken(elapsed_time))\n",
    "\n",
    "    if savemodel_interval == 0 and savemodel:\n",
    "        torch.save(cnn.state_dict(), \n",
    "           '{}/{:%Y-%m-%d_%H-%M-%S}_model_epoch{}.pth'.format(savemodeldir,datetime.now(),num_epochs))\n",
    "        print('model saved at epoch{}'.format(num_epochs))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    \n",
    "    args = parse_args()\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e526740d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import math \n",
    "\n",
    "\n",
    "os.chdir('/groups/cherkasvgrp/share/progressive_docking/hmslati/plif_cnn')\n",
    "\n",
    "with open(\"zero_train_zfeats.pkl\",'rb') as f:    #train_data_2.pkl\n",
    "    train_data=pickle.load(f)\n",
    "\n",
    "all_labels=[]\n",
    "for i in train_data:\n",
    "    with open(i[1],'rb') as f2: all_labels.append(round(-1 * math.log(pickle.load(f2)),1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c008dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd \n",
    "\n",
    "df = pd.DataFrame({'target': all_labels})\n",
    "df['target'] = df['target'].astype('category')\n",
    "class_count_df = df.groupby(all_labels).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3cc971a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9013"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_count_df.iloc[:,0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2367215d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    df = pd.DataFrame({'target': all_labels})\n",
    "    df['target'] = df['target'].astype('category')\n",
    "    class_count_df = df.groupby(all_labels).count()\n",
    "    class_weight=[]\n",
    "    def calc_weight(idx,n_1):\n",
    "        n_0 = class_count_df.iloc[idx, 0]\n",
    "\n",
    "        return (n_1) / (272 * n_0)\n",
    "\n",
    "    for x in range(272):\n",
    "        class_weight.append(calc_weight(x,class_count_df.iloc[:,0].sum()))\n",
    "\n",
    "    class_weights=torch.FloatTensor(class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "91a265d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35251095118898623"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weight[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49c6541f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Important: Convert Weights To Float Tensor\n",
    "class_weights=torch.FloatTensor(class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d98a082d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([33.1360, 33.1360, 33.1360, 16.5680, 33.1360, 33.1360, 33.1360, 33.1360,\n",
       "        33.1360, 16.5680, 16.5680, 11.0453, 16.5680, 16.5680, 16.5680, 16.5680,\n",
       "        11.0453, 11.0453, 11.0453,  5.5227,  5.5227,  8.2840, 11.0453, 16.5680,\n",
       "        11.0453, 11.0453,  3.3136,  6.6272,  6.6272,  4.7337,  2.5489,  2.2091,\n",
       "         6.6272,  1.6568,  6.6272,  3.0124,  8.2840,  1.9492,  3.6818,  8.2840,\n",
       "         2.7613,  6.6272,  1.8409,  3.6818,  2.0710,  2.0710,  1.2273,  1.5779,\n",
       "         2.7613,  1.3807,  1.5062,  1.6568,  1.9492,  0.8956,  1.1834,  1.7440,\n",
       "         0.8720,  1.3254,  1.1426,  1.8409,  1.1426,  0.8720,  1.1426,  0.6762,\n",
       "         1.0041,  0.7050,  1.1045,  0.8082,  1.5062,  0.7364,  1.0689,  0.8720,\n",
       "         0.6136,  0.7050,  0.7531,  0.8496,  0.6372,  0.5917,  0.8496,  0.4734,\n",
       "         0.8720,  0.7364,  0.5813,  0.5021,  0.4303,  0.6497,  0.4303,  0.5523,\n",
       "         0.3853,  0.5523,  0.5260,  0.6627,  0.5178,  0.4091,  0.6025,  0.4360,\n",
       "         0.4478,  0.3809,  0.5260,  0.4946,  0.3525,  0.5178,  0.3945,  0.5345,\n",
       "         0.3525,  0.5098,  0.4602,  0.4478,  0.5616,  0.5098,  0.5178,  0.3314,\n",
       "         0.4142,  0.4539,  0.5813,  0.5917,  0.3314,  0.5523,  0.4041,  0.4360,\n",
       "         0.3853,  0.5813,  0.4418,  0.3992,  0.6903,  0.3525,  0.5345,  0.3809,\n",
       "         0.5616,  0.4734,  0.3314,  0.4802,  0.3682,  0.5432,  0.3563,  0.4360,\n",
       "         0.4802,  0.4667,  0.6252,  0.4303,  0.8496,  0.4041,  0.5523,  0.6136,\n",
       "         0.5432,  0.3602,  0.4091,  0.5917,  0.4539,  0.4946,  0.3641,  0.4802,\n",
       "         0.5021,  0.4667,  0.4478,  0.4418,  0.5917,  0.4194,  0.6627,  0.6903,\n",
       "         0.4734,  0.6762,  0.4248,  0.6252,  0.4946,  0.6903,  0.6497,  0.8082,\n",
       "         0.6025,  0.5098,  1.0689,  0.6497,  0.9467,  0.4802,  1.1426,  1.1834,\n",
       "         0.5432,  0.9746,  0.8956,  1.2273,  0.4303,  1.1426,  1.3807,  0.8956,\n",
       "         2.0710,  0.9746,  1.3807,  0.6762,  1.5062,  0.8284,  3.3136,  1.2273,\n",
       "         1.1426,  2.0710,  0.9746,  2.5489,  1.2273,  1.9492,  1.9492,  1.1045,\n",
       "         1.9492,  5.5227,  2.2091,  1.0041,  2.5489,  4.1420,  1.6568,  4.1420,\n",
       "         2.0710,  3.6818,  1.3254,  5.5227,  4.7337, 16.5680,  2.5489,  2.7613,\n",
       "         4.1420,  3.3136,  4.7337,  4.7337,  6.6272,  8.2840,  3.0124,  3.6818,\n",
       "         5.5227,  8.2840,  4.1420,  4.7337, 33.1360,  3.0124, 11.0453,  6.6272,\n",
       "         5.5227,  8.2840, 11.0453,  6.6272, 16.5680, 33.1360,  5.5227, 11.0453,\n",
       "         3.6818,  5.5227, 33.1360, 33.1360, 16.5680,  6.6272,  8.2840, 33.1360,\n",
       "        33.1360,  5.5227, 16.5680,  8.2840, 33.1360, 11.0453,  6.6272, 33.1360,\n",
       "        16.5680, 33.1360, 11.0453, 33.1360, 16.5680, 16.5680, 33.1360,  6.6272,\n",
       "        33.1360, 33.1360, 33.1360, 33.1360, 33.1360, 33.1360, 33.1360, 33.1360])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
