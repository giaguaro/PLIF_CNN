{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbbe9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This module defines a python wrapper function for stacked ensemble regression model based on sklearn machin learning library.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import os, pickle\n",
    "import csv\n",
    "from itertools import combinations\n",
    "from sklearn.base import clone\n",
    "from sklearn import preprocessing\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "__author__ = \"Zheng Li\"\n",
    "__email__ = \"zhengl@vt.edu\"\n",
    "__date__ = \"Nov. 26, 2019\"\n",
    "\n",
    "class stacked_ensemble_regression():\n",
    "    def __init__(self, sub_estimator, aggregator_estimator, feature_name, layers, model_number_layer, feature_ratio, sample_ratio,random_state):\n",
    "        self.sub_estimator = {}\n",
    "        for es in sub_estimator.keys():\n",
    "            self.sub_estimator[es] = clone(sub_estimator[es])\n",
    "        \n",
    "        self.aggregator_estimator = {}\n",
    "        for es in aggregator_estimator.keys():\n",
    "            self.aggregator_estimator[es] = clone(aggregator_estimator[es])\n",
    "\n",
    "        self.feature_name = feature_name\n",
    "        self.columns = feature_name\n",
    "        self.layers = layers\n",
    "        self.model_number_layer = model_number_layer\n",
    "        self.feature_ratio = feature_ratio\n",
    "        self.sample_ratio = 1- sample_ratio\n",
    "        self.random_state = random_state\n",
    "        self.path = os.getcwd()\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Optimize the ensemble model parameters in the \"sand box\" layers with the training data.\n",
    "        \"\"\"\n",
    "        # load the data in panda data framework\n",
    "        X_df = pd.DataFrame(X, columns = self.feature_name)\n",
    "        # delete the previous model parameters file and create a new one\n",
    "        if os.path.exists(self.path + '/' + 'model_params'):\n",
    "            os.system('rm -r model_params')\n",
    "        os.mkdir('model_params')\n",
    "        # enumerate and train all the sub-models in each layer \n",
    "        n = 0\n",
    "        while n < self.layers:\n",
    "            dir = self.path + '/model_params' + '/layer_' + str(n+1)\n",
    "            os.mkdir(dir)\n",
    "            num = self.model_number_layer[n]\n",
    "            feature_gen = []\n",
    "            feature_names = []\n",
    "            DATA_params = {}\n",
    "            for m in range(num):\n",
    "                # select a random feature size according to the pre-defined ratio (\"feature_ratio\")\n",
    "                columns_select = random.sample(self.feature_name, int(round(len(self.feature_name)* self.feature_ratio)))\n",
    "                print ('columns_select', columns_select)\n",
    "                if n == 0:\n",
    "                    X_tr = X_df\n",
    "                    X_tr = X_tr[columns_select].values.astype(np.float)\n",
    "                else:\n",
    "                    X_tr = self.X_tr[columns_select].values.astype(np.float)\n",
    "                # feature preprocessing to standarize the feature for an improvement of training performance  \n",
    "                scaler = preprocessing.StandardScaler()\n",
    "                scaler.fit(X_tr)\n",
    "                X_tr = scaler.transform(X_tr)\n",
    "                # save the preprocessing parameters for prediction\n",
    "                DATA_params[m] = {'mean' : scaler.mean_, 'variance': scaler.var_, 'columns': columns_select}\n",
    "                # select a random sample size according to the pre-defined ratio (\"sample_ratio\") \n",
    "                X_train, X_test, Y_train, Y_test = train_test_split(X_tr, Y,\\\n",
    "                                                      test_size = self.sample_ratio, random_state= self.random_state)\n",
    "                # optimize all the sub_model parameters \n",
    "                for es in self.sub_estimator:\n",
    "                    self.sub_estimator[es].fit(X_train, Y_train.ravel())\n",
    "                    joblib.dump(self.sub_estimator[es], dir+ '/' + es + '_' + str(m)+'.pkl')\n",
    "                    feature = self.sub_estimator[es].predict(X_tr)\n",
    "                    feature_gen.append(feature)\n",
    "                    feature_names.append(es + '_' + str(m))\n",
    "            # update 'X_tr' data for training the models at next layer  \n",
    "            self.feature_name = feature_names\n",
    "            self.X_tr = pd.DataFrame(np.array(feature_gen).T, columns = self.feature_name)\n",
    "            # save the model parameters in pickle file\n",
    "            output = open(dir+ '/' + 'params.pkl','wb')\n",
    "            pickle.dump(DATA_params, output)\n",
    "            output.close()\n",
    "            n+=1\n",
    "        # create folder for aggregation model\n",
    "        self.dir_aggregator = self.path + '/model_params/' + 'aggregator'\n",
    "        os.mkdir(self.dir_aggregator)\n",
    "        # train the aggregator model using the data from the last layer\n",
    "        for es in self.aggregator_estimator:\n",
    "            self.aggregator_estimator[es].fit(self.X_tr, Y.ravel())\n",
    "            joblib.dump(self.aggregator_estimator[es], self.dir_aggregator + '/' + es + '.pkl')\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Ensemble model prediction using the trained model architectures. \n",
    "        \"\"\"\n",
    "        X_df = pd.DataFrame(X, columns = self.columns)\n",
    "        if os.path.exists(self.path + '/' + 'model_params'):\n",
    "            n = 0\n",
    "            while n < self.layers:\n",
    "                dir = self.path + '/model_params' + '/layer_' + str(n+1)\n",
    "                # load in all the trained model parameters from pickle file\n",
    "                f = open(dir + '/' + 'params.pkl', 'rb')\n",
    "                u = pickle._Unpickler(f)\n",
    "                u.encoding = 'latin1'\n",
    "                DATA_params = u.load()\n",
    "                f.close()\n",
    "                # load in the feature columns at each layer  \n",
    "                feature_names = []\n",
    "                feature_gen = []\n",
    "                num = self.model_number_layer[n]\n",
    "                for m in range(num):\n",
    "                    columns_select = DATA_params[m]['columns']\n",
    "                    print('columns_select', columns_select)\n",
    "                    if n == 0:\n",
    "                        X_ = X_df\n",
    "                        X_ = X_[columns_select].values.astype(np.float)\n",
    "                    else:\n",
    "                        X_ = self.X_te[columns_select].values.astype(np.float)\n",
    "                    # load in the standarization parameters\n",
    "                    X_ = (X_ - DATA_params[m]['mean'])/np.sqrt(DATA_params[m]['variance'])\n",
    "                    # model prediction on the new data\n",
    "                    for es in self.sub_estimator:\n",
    "                        sub_estimator = joblib.load(dir+ '/' + es+ '_' + str(m)+'.pkl')\n",
    "                        feature = sub_estimator.predict(X_)\n",
    "                        feature_gen.append(feature)\n",
    "                        feature_names.append(es+ '_' + str(m))\n",
    "                # update 'X_te' data for training the models at next layer  \n",
    "                self.X_te = pd.DataFrame(np.array(feature_gen).T, columns = feature_names)\n",
    "                n+=1\n",
    "            # aggregator model prediction\n",
    "            for es in self.aggregator_estimator:\n",
    "                aggregator_model = joblib.load(self.dir_aggregator + '/' + es + '.pkl')\n",
    "                prediction = aggregator_model.predict(self.X_te)\n",
    "        else:\n",
    "            raise ValueError('Invalid model parameter file or model_params file is missing')\n",
    "     \n",
    "        return prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
